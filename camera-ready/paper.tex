\documentclass[sigconf]{acmart}

\copyrightyear{2017}
\acmYear{2017}
\setcopyright{acmcopyright}
\acmConference{ICMR '17}{June 06-09, 2017}{Bucharest,
Romania}\acmPrice{15.00}\acmDOI{http://dx.doi.org/10.1145/3078971.3078996}
\acmISBN{978-1-4503-4701-3/17/06}


\usepackage{booktabs} % For formal tables
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{algorithmic}
\usepackage[bf, medium]{titlesec}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\algorithmicrequire}{\textbf{input:}}
\renewcommand{\algorithmicensure}{\textbf{output:}}
% for saving space
\setlength{\abovecaptionskip}{1pt}
\setlength{\belowcaptionskip}{2pt}
\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{0pt}
\setlength{\floatsep}{1pt}
\setlength{\dbltextfloatsep}{1pt}
\setlength{\dblfloatsep}{1pt}
\titlespacing\section{0pt}{7pt minus 2pt}{0 pt plus 2pt}
\titlespacing\subsection{0pt}{7pt minus 2pt}{0pt plus 2pt} 
\titlespacing\subsubsection{0pt}{7pt minus 2pt}{0pt plus 2pt}


\begin{document}
\title{Joint Saliency Estimation and Matching using Image Regions for Geo-Localization of Online Video}

\author{Haoyue Shi}
\affiliation{
  \institution{School of Electronics Engineering and Computer Science, Peking University}
  \streetaddress{No.5 Yiheyuan Road}
  \city{Beijing} 
  \country{P. R. China} 
  \postcode{100871}
}
\email{hyshi@pku.edu.cn}

\author{Jia Chen}
\affiliation{
  \institution{Language Technologies Institute, Carnegie Mellon University}
  \streetaddress{5000 Forbes Avenue}
  \city{Pittsburgh} 
  \state{PA}
  \country{USA} 
  \postcode{15213}
}
\email{jiac@cs.cmu.edu}
\authornote{Jia Chen is the corresponding author}


\author{Alexander G. Hauptmann}
\affiliation{
  \institution{Language Technologies Institute, Carnegie Mellon University}
  \streetaddress{5000 Forbes Avenue}
  \city{Pittsburgh} 
  \state{PA}
  \country{USA} 
  \postcode{15213}
}
\email{alex@cs.cmu.edu}
\renewcommand{\shorttitle}{JSEM using Image Regions for Geo-Localization for Online Video}

\begin{abstract}
In this paper, we study automatic geo-localization of online event videos. 
Different from general image localization task through matching, the appearance of an environment during significant events varies greatly from its daily appearance, since there are usually crowds, decorations or even destruction when a major event happens. 
This introduces a major challenge: matching the event environment to the daily environment, e.g. as recorded by Google Street View. 
We observe that some regions in the image, as part of the environment, still preserve the daily appearance even though the whole image (environment) looks quite different. 
Based on this observation, we formulate the problem as joint saliency estimation and matching at the image region level, as opposed to the key point or whole-image level. 
As image-level labels of daily environment are easily generated with GPS information, we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data. 
Our solution is to iteratively optimize saliency and the region-matching model. 
For saliency optimization, we derive a closed form solution, which has an intuitive explanation. 
For region matching model optimization, we use self-paced learning to learn from the pseudo labels generated by (sub-optimal) saliency values. 
We conduct extensive experiments on two challenging public datasets: Boston Marathon 2013 and Tokyo Time Machine. 
Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas. 
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003371.10003386</concept_id>
<concept_desc>Information systems~Multimedia and multimodal retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003386.10003387</concept_id>
<concept_desc>Information systems~Image search</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003386.10003388</concept_id>
<concept_desc>Information systems~Video search</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Multimedia and multimodal retrieval}
\ccsdesc[300]{Information systems~Image search}
\ccsdesc[300]{Information systems~Video search}
\ccsdesc[300]{Computing methodologies~Neural networks}

\keywords{Video Geo-Localization, Region Saliency, Region Matching}

\maketitle

\input{introduction}
\input{related}
\input{problem}
\input{solution}
\input{expr}
\input{conclusion}

\section{Acknowledgement}
This material is based in part upon work supported by the National Science Foundation under Grant Number IIS- 1638429. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

% \newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sigproc} 

\end{document}
