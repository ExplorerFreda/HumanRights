\documentclass[sigconf]{acmart}

\copyrightyear{2017} 
\acmYear{2017} 
\setcopyright{acmcopyright}
\acmConference{ICMR '17}{}{June 6--9, 2017, Bucharest, Romania}
\acmPrice{15.00.}
\acmDOI{http://dx.doi.org/10.1145/XXXXXXX.XXXXXXX}
\acmISBN{ISBN 978-1-4503-4701-3/17/06} 
%Authors, replace the red X's with your assigned DOI string. See pdf attached to ACM rightsreview confirmation email.

\usepackage{booktabs} % For formal tables
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{algorithmic}
\usepackage[bf, medium]{titlesec}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\algorithmicrequire}{\textbf{input:}}
\renewcommand{\algorithmicensure}{\textbf{output:}}
% for saving space
\setlength{\abovecaptionskip}{1pt}
\setlength{\belowcaptionskip}{2pt}
\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{0pt}
\setlength{\floatsep}{1pt}
\setlength{\dbltextfloatsep}{1pt}
\setlength{\dblfloatsep}{1pt}
\titlespacing\section{0pt}{7pt minus 2pt}{0 pt plus 2pt}
\titlespacing\subsection{0pt}{7pt minus 2pt}{0pt plus 2pt} 
\titlespacing\subsubsection{0pt}{7pt minus 2pt}{0pt plus 2pt}


\begin{document}
\title{Joint Saliency Estimation and Matching using Image Regions for Geo-Localization of Online Video}

\author{Haoyue Shi}
\affiliation{
  \institution{School of EECS, Peking University}
  \streetaddress{No.5 Yiheyuan Road}
  \city{Beijing} 
  \country{P. R. China} 
  \postcode{100871}
}
\email{hyshi@pku.edu.cn}

\author{Jia Chen}
\affiliation{
  \institution{Language Technologies Institute, Carnegie Mellon University}
  \streetaddress{5000 Forbes Avenue}
  \city{Pittsburgh} 
  \state{PA}
  \country{USA} 
  \postcode{15213}
}
\email{jiac@cs.cmu.edu}


\author{Alexander G. Hauptmann}
\affiliation{
  \institution{Language Technologies Institute, Carnegie Mellon University}
  \streetaddress{5000 Forbes Avenue}
  \city{Pittsburgh} 
  \state{PA}
  \country{USA} 
  \postcode{15213}
}
\email{alex@cs.cmu.edu}
\renewcommand{\shorttitle}{JSEM using Image Regions for Geo-Localization for Online Video}


\begin{abstract}
In this paper, we study automatic geo-localization of online event videos, which is a key component behind various event analysis tasks such as cross-camera tracking and 3D reconstruction from co-located videos generated by multiple users. 
Different from general image localization task through matching, the appearance of an environment during significant events varies greatly from its daily appearance, since there are usually crowds, decorations, obstructions or even destruction when a major event happens. 
This introduces a major challenge: matching the event environment to the daily environment, e.g. as recorded by Google Street View. 
We observe that some regions in the image, as part of the environment, still preserve the daily appearance even though the whole image (environment) looks quite different. 
Based on this observation, we formulate the problem as joint saliency estimation and matching at the image region level, as opposed to the key point or whole-image level. 
As image-level labels of daily environment are easily generated with GPS information, we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data. 
Our solution is to iteratively optimize saliency and the region-matching model. 
For saliency optimization, we derive a closed form solution, which has an intuitive explanation. 
For region matching model optimization, we use self-paced learning to learn from the pseudo labels generated by (sub-optimal) saliency values. 
In the test stage, we retrieve daily environment images for each frame in the video based on an image level similarity score calculated by region saliency and matching. 
We combine the GPS information of the retrieved and matched images to get the final geo-location for the video. 
We conduct extensive experiments on two challenging public datasets: Boston Marathon 2013 and Tokyo Time Machine. 
Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas. 
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003371.10003386</concept_id>
<concept_desc>Information systems~Multimedia and multimodal retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003386.10003387</concept_id>
<concept_desc>Information systems~Image search</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003386.10003388</concept_id>
<concept_desc>Information systems~Video search</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010542.10010294</concept_id>
<concept_desc>Computer systems organization~Neural networks</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Multimedia and multimodal retrieval}
\ccsdesc[300]{Information systems~Image search}
\ccsdesc[300]{Information systems~Video search}
\ccsdesc[100]{Computer systems organization~Neural networks}

\keywords{Video Geo-Localization, Region Saliency, Region Matching}

\maketitle

\input{introduction}
\input{related}
\input{problem}
\input{solution}
\input{expr}
\input{conclusion}

\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sigproc} 

\end{document}
